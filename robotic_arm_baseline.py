# -*- coding: utf-8 -*-
"""Robotic_arm_baseline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NJAz-Q6Cgo0QjA0W-RFku8Yjn52NVUj6
"""

!pip install stable-baselines3



import gym
from stable_baselines3 import A2C
from stable_baselines3.common.vec_env import DummyVecEnv
import matplotlib.pyplot as plt

# Define the CartPole environment
env = gym.make('CartPole-v1')

# Wrap the environment in a DummyVecEnv object
env = DummyVecEnv([lambda: env])

# Define the A2C agent and train it on the environment
agent = A2C('MlpPolicy', env, verbose=1)
rewards = []
mean_rewards = []
for i in range(500):
    obs = env.reset()
    done = False
    episode_rewards = []
    while not done:
        action, _ = agent.predict(obs)
        obs, reward, done, info = env.step(action)
        episode_rewards.append(reward)
    rewards.append(sum(episode_rewards))
    mean_rewards.append(sum(rewards) / len(rewards))

# Plot the total reward across episodes
plt.plot(mean_rewards)
plt.xlabel('Episode')
plt.ylabel('Mean reward')
plt.show()



import gym
from stable_baselines3 import A2C
import os

os.environ["SDL_VIDEODRIVER"] = "dummy"

# Create the CartPole environment
env = gym.make("CartPole-v1")

# Train the A2C model on the environment
model = A2C("MlpPolicy", env, verbose=1)
model.learn(total_timesteps=50000)

# Test the trained model
obs = env.reset()
while True:
    action, _states = model.predict(obs)
    obs, rewards, dones, info = env.step(action)
#    env.render()
    if dones:
        break

# Close the environment
env.close()

model.save("a2c_cartpole-v1")

! pip install huggingface_sb3

!pip install huggingface_hub

!huggingface-cli login

!git config --global credential.helper store

! huggingface-cli repo create TP1_RL_ELMAI --type model

!pip install wandb tensorboard
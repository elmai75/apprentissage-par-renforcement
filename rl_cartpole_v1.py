# -*- coding: utf-8 -*-
"""RL_Cartpole_V1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UykhK2EH0emtftWswK2ydizEStqH6xsD
"""

import gym
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import matplotlib.pyplot as plt

# Definition de l'environement Cartpole 
env = gym.make('CartPole-v1')

# Definition de réseaux de neuron
class Policy(nn.Module):
    def __init__(self):
        super(Policy, self).__init__()
        self.fc1 = nn.Linear(4, 128)
        self.dropout = nn.Dropout(p=0.6)
        self.fc2 = nn.Linear(128, 2)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return F.softmax(x, dim=1)

policy = Policy()
optimizer = optim.Adam(policy.parameters(), lr=5e-3)

# la boucle de l'entrainement
rewards = []
for episode in range(500):
    state = env.reset()
    log_probs = []
    rewards_episode = []

    while True:
        state_tensor = torch.from_numpy(state).float().unsqueeze(0)
        action_probs = policy(state_tensor)
        m = torch.distributions.Categorical(action_probs)
        action = m.sample()
        log_probs.append(m.log_prob(action))
        state, reward, done, _ = env.step(action.item())
        rewards_episode.append(reward)
        if done:
            break

    returns = []
    G = 0
    for r in rewards_episode[::-1]:
        G = r + 0.99*G
        returns.insert(0, G)

    returns = torch.tensor(returns)
    returns = (returns - returns.mean()) / (returns.std() + 1e-9)

    policy_loss = []
    for log_prob, G in zip(log_probs, returns):
        policy_loss.append(-log_prob * G)

    policy_loss1 = torch.cat(policy_loss).sum()

    optimizer.zero_grad()
    policy_loss1.backward()
    optimizer.step()

    total_reward = sum(rewards_episode)
    rewards.append(total_reward)

# Traçage de l'évolution des recompenses en fonction des épisodes 
plt.plot(rewards)
plt.xlabel('Episode')
plt.ylabel('Total reward')
plt.show()


# -*- coding: utf-8 -*-
"""TP1_RL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1saQYthSGwXhRl9YZvBnLyw6kE_ir7HqV
"""

import gym
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import matplotlib.pyplot as plt

# Definition de l'environement Cartpole 
env = gym.make('CartPole-v1')

# Definition de réseaux de neuron
class Policy(nn.Module):
    def __init__(self):
        super(Policy, self).__init__()
        self.fc1 = nn.Linear(4, 128)
        self.dropout = nn.Dropout(p=0.6)
        self.fc2 = nn.Linear(128, 2)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return F.softmax(x, dim=1)

policy = Policy()
optimizer = optim.Adam(policy.parameters(), lr=5e-3)

# la boucle de l'entrainement
rewards = []
for episode in range(500):
    state = env.reset()
    log_probs = []
    rewards_episode = []

    while True:
        state_tensor = torch.from_numpy(state).float().unsqueeze(0)
        action_probs = policy(state_tensor)
        m = torch.distributions.Categorical(action_probs)
        action = m.sample()
        log_probs.append(m.log_prob(action))
        state, reward, done, _ = env.step(action.item())
        rewards_episode.append(reward)
        if done:
            break

    returns = []
    G = 0
    for r in rewards_episode[::-1]:
        G = r + 0.99*G
        returns.insert(0, G)

    returns = torch.tensor(returns)
    returns = (returns - returns.mean()) / (returns.std() + 1e-9)

    policy_loss = []
    for log_prob, G in zip(log_probs, returns):
        policy_loss.append(-log_prob * G)

    policy_loss1 = torch.cat(policy_loss).sum()

    optimizer.zero_grad()
    policy_loss1.backward()
    optimizer.step()

    total_reward = sum(rewards_episode)
    rewards.append(total_reward)

# Traçage de l'évolution des recompenses en fonction des épisodes 
plt.plot(rewards)
plt.xlabel('Episode')
plt.ylabel('Total reward')
plt.show()

!pip install stable-baselines3

import gym
from stable_baselines3 import A2C
from stable_baselines3.common.vec_env import DummyVecEnv
import matplotlib.pyplot as plt

# Define the CartPole environment
env = gym.make('CartPole-v1')

# Wrap the environment in a DummyVecEnv object
env = DummyVecEnv([lambda: env])

# Define the A2C agent and train it on the environment
agent = A2C('MlpPolicy', env, verbose=1)
rewards = []
mean_rewards = []
for i in range(500):
    obs = env.reset()
    done = False
    episode_rewards = []
    while not done:
        action, _ = agent.predict(obs)
        obs, reward, done, info = env.step(action)
        episode_rewards.append(reward)
    rewards.append(sum(episode_rewards))
    mean_rewards.append(sum(rewards) / len(rewards))

# Plot the total reward across episodes
plt.plot(mean_rewards)
plt.xlabel('Episode')
plt.ylabel('Mean reward')
plt.show()

import gym
from stable_baselines3 import A2C
import os

os.environ["SDL_VIDEODRIVER"] = "dummy"

# Create the CartPole environment
env = gym.make("CartPole-v1")

# Train the A2C model on the environment
model = A2C("MlpPolicy", env, verbose=1)
model.learn(total_timesteps=50000)

# Test the trained model
obs = env.reset()
while True:
    action, _states = model.predict(obs)
    obs, rewards, dones, info = env.step(action)
#    env.render()
    if dones:
        break

# Close the environment
env.close()

model.save("a2c_cartpole-v1")

! pip install huggingface_sb3

!pip install huggingface_hub



!huggingface-cli login

!git config --global credential.helper store

! huggingface-cli repo create TP1_RL_ELMAI --type model

!git lfs install
!git clone https://huggingface.co/Mohamed10/TP1_RL_ELMAI